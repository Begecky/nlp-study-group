# Lecture 2: Word Vector Representations: word2vec

# Lecture Plan
- Lecture plan: [2:28](https://www.youtube.com/watch?v=ERibwqs9p38&t=2m28s)
- Word meaning: [4:20](https://www.youtube.com/watch?v=ERibwqs9p38&t=4m20s)
- Word2vec:
- Reseach highlight:
- Word2vect objective function gradients
- Optimization refresher:
- Assignment 1 notes
- Usefulness of word2vec:

# Highlights
- Distributed representation of meaning [10:54](https://www.youtube.com/watch?v=ERibwqs9p38&t=10m54s)
  - one-hot encoding 
  - localist, no inherent notion between words
  - denotational. 'right here' 
 
- Distributional Similarity [15:00](https://www.youtube.com/watch?v=ERibwqs9p38&t=15m0s)
  - Wittgenstein's "use theory of meaning"
  - consider a word's use in context
  - You shal know a word by the company it keeps." J.R. Firth
  - not the same as distributed representation of meaning
  - create a dense vector that will dot-product nicely to suggest similarity
  
- 
# References
- [wittgenstein](https://plato.stanford.edu/entries/wittgenstein/)
